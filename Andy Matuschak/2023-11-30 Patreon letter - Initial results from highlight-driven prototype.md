---
URLs:
  - https://notes.andymatuschak.org/zY3RYK9gJ6eDnq27vSwBDQh?stackedNotes=z9V2PxHVYB9p5DeCnQXcfJa
---
# 2023-11-30 Patreon letter - Initial results from highlight-driven prototype
_Private copy; not to be shared publicly; part ofÂ [[Patron letters on memory system experiments]]_

[[2023-09-30 Patreon letter - Highlight-driven practice and comprehension support|Iâ€™ve been working on]]Â a new augmented reading environment centered aroundÂ _highlighting_Â as the core interaction. The idea is to give readers a magic wand with two unusual â€œpowersâ€:
1. You can point the wand at anything and say â€œmake sure I know this.â€
2. You can wave the wand over a section and ask â€œdid I miss anything important?â€

Of course, those are aspirational framings. My current design instantiates those powers like this:

1. You have a special purple highlighter. When you mark text with it, spaced repetition prompts about those ideas will be added to future review sessions.
2. At the end of each section, you can press a button to take a second pass over â€œsuggested highlightsâ€. This button marks phrases corresponding to all the details which the author thought were important, but whichÂ _didnâ€™t_Â semantically intersect your own highlights. You can scroll through to quickly check for gaps in your reading comprehension.

Itâ€™s easy to imagine more elaborate instantiations! These are intended as a meaningful first step towards the more aspirational powers. For more background,Â [[2023-09-30 Patreon letter - Highlight-driven practice and comprehension support|see my introductory letter]]Â on the concept.

## An initial prototype

This month, I tested a prototype of this concept, adaptingÂ [a linear algebra primer by Jim Hefferon](https://hefferon.net/linearalgebra/index.html) #external . You can see the prototype in action inÂ **[this new demo video](https://youtu.be/n3ugRmARpYw)**Â (6m17s) #external . I hosted study sessions with 14 people who had some authentic reason to study linear algebra, and who had some experience with spaced repetition memory systems. We met in person, one-on-oneâ€”that always helps me form richer impressions of a new prototype.

After a short background interview, and an explanation of the interface, participants read the first section of the book, marking it with both a normal yellow highlighter and the special purple highlighter however they liked. At the end of the section, we used the â€œsuggested highlightsâ€ tool to make a quick second pass. I asked readers to comment on each extra highlight: was it something they understood but didnâ€™t feel was worth marking, or was it something they skimmed over? Finally, we reviewed all the prompts corresponding to their purple highlights. I probed readers about how it felt to review these prompts, and whether they felt their highlights were faithfully represented.

Before we dig into what I observed, I should explain that this prototype involved some significant smoke and mirrors. Readers imagined that Iâ€™d implemented some elaborate machine learning system. But noâ€”not yet, anyway. Hereâ€™s how it worked:

- Before meeting with any readers, I manually â€œcuratedâ€ highlights corresponding to what I thought were all the important details in the section we read.
- For each of those highlights, I wrote one or more practice prompts.
- Then, in realtime while each participant read, I manually mapped each of their highlights onto the curated highlights (if any) which pointed at the same underlying idea. This sometimes required fluid judgment!
- The â€œsuggested highlightâ€ feature then displayed all my curated highlights which had no corresponding reader highlights.
- The review session displayed the prompts associated with all the curated highlights Iâ€™d mapped to the readerâ€™s highlights.

That may look fairly baroque in writing, but in practice it created a remarkably transparent experience. Readers didnâ€™t perceive manual steps; they often innocently asked if they could keep using the tool to read on their own after our session. This â€œWizard of Ozâ€-style testing let me focus on the interaction design concepts, rather than on potentially unbounded problems of language model pipelines. Thatâ€™s the right trade to be making for now.

## What I learned from readers
Pre-screening is always imperfect in user research. For 3 of my 14 participants, the book (intended as a first course for undergraduates) was too difficult to read comfortably. Another 2 readers didnâ€™t actually want to understand the topic in detail; they just wanted â€œthe big pictureâ€. In discussing what I learned, Iâ€™ll focus on the 9 readers who aligned with my target audience.

### Mapping highlights to prompts seems very promising
Readers broadly loved the concept of the augmented highlighting interaction. Most of them already had a habit of highlighting texts, though all readily admitted that they didnâ€™t think it actually affected how well they learned the material. Instead, readers described highlighting as a fidgeting behavior, a way to stay more engaged, and an ad-hoc bookmarking method. One reader didnâ€™t end up using the special highlighter; he self-described as hypermnesic and felt he didnâ€™t need practice support for the sectionâ€™s material. The rest used the highlighter extensively.

Most readers were extremely happy with the retrieval practice prompts they were given. One said: â€œThe promptsÂ captured my own intent to the point where it took me some time to realize that they werenâ€™t written by meâ€. Another said: â€œThese are the kinds of things I wish I could actually have with a highlighting system. â€¦ Itâ€™s not just throwing things back at me verbatim. â€¦ For the concepts I highlighted, it asked me about the important logical relationships.â€ Most readers spontaneously asked if they could use â€œthe magic highlighterâ€ with other books.

Interestingly, readers had such positive perceptions of my highlight-to-prompt mappingsÂ _despite_Â the fact that I hadnâ€™t always prepared a corresponding prompt for each of their highlights. Most readers highlighted a couple points which I didnâ€™t feel were important enough to mark. No one noticed the absences, but I worry that this kind of silent omission would erode trust in the tool over time. Fully fixing this problem would require reliable machine-generated prompts; absent that, we could provide a fallback workflow for readers to write their own prompts for these â€œmissingâ€ highlights.

As I hoped, this design also mostly eliminated two failure modes I saw routinely in past mnemonic medium prototypes. Because each prompt (in principle) corresponded to something which the reader â€œsaidâ€ they wanted to know, readers were much less likely to experience the review sessions asÂ [[2021-08-24 Patreon letter - Revamping the mnemonic medium around reader control|unpleasantly authoritarian or â€œschool-likeâ€]]. And for the same reasonâ€”in conjunction with the reading comprehension support mechanismâ€”readers were less oftenÂ outright confused by a question or its answer. Of course, readers could (and did) highlight passages without really understanding them, but when that happened, readers didnâ€™t complain of those prompts as feeling arbitrary and capricious as they did in previous prototypes. The review interface offers a â€œView Sourceâ€ button which shows the connection to the source material theyâ€™d highlighted, for any prompt. I think this generally created a feeling that readers had â€œasked forâ€ their confusion, rather than that the confusion was â€œbeing done toâ€ them.

These sessions werenâ€™t a rigorous experiment; I was only aiming for high-level qualitative evaluation. But my early impression is that the prompts-from-highlights design concept seems fundamentally quite promising, and is well worth pushing further.

### Suggested highlights diagnosed some gaps, felt lightweight

This prototypeâ€™s second big idea was â€œsuggested highlightsâ€ as a lightweight reading comprehension support intervention. Here results were somewhat more equivocal. The readers I worked with varied enormously in their pace and diligence. Some muttered every word under their breath, stopped routinely to ask and answer questions of the text, and re-read passages multiple times to clarify misunderstandings. Others breezed through in less than half the time, skipping passages which seemed repetitive or obvious. The â€œtestingâ€ context created distortions, too: some readers confessed that they were reading much more carefully than they would if I werenâ€™t presentâ€”even though I explicitly asked them not to as part of their initial instructions.

Of the 9 readers matching my intended target user, 4 had meaningful reading comprehension gaps. The â€œsuggested highlightsâ€ interaction quickly identified places where these readers hadnâ€™t attended to some important point, and gave them a straightforward opportunity to fill that gap. Sometimes readers felt the details they missed werenâ€™t so important, but often readers colored the â€œsuggested highlightsâ€ with their special purple highlighter once theyâ€™d re-read the passage. I take that as a sign that the interaction identified something meaningful.

These readers were quite enthusiastic about the design. One said: â€œThis is the tool that I want!â€ Another: â€œThis is insanely cool! Man, I wish I had this everywhere.â€ One hesitation I have is that if these readers had a few straightforward gaps which my tool could identify, they probably had some other subtle gaps which will remain. Maybe itâ€™s fine. Maybe these are the kinds of details which will get easily ironed out during a problem set. And the interaction at least ensured that readers werenâ€™t being asked to do retrieval practice on material they hadnâ€™t understoodâ€”[[2023-09-30 Patreon letter - Highlight-driven practice and comprehension support|a key goal]]. Iâ€™ll need to run more focused experiments to better understand the effects of my intervention on reading comprehension.

The other 5 â€œtargetâ€ users had no overt comprehension gaps; the â€œsuggested highlightsâ€ were all false positives. I spontaneously probed these readersâ€™ understandings with extra questions, and they all performed quite well. So these readers didnâ€™tÂ _need_Â extra reading comprehension support, at least in the test context. Happily, 3 of these 5 liked theÂ _idea_Â of the tool, and said that they didnâ€™t mind the false positives; they found the interaction lightweight enough that they would want to use it anyway: â€œI still think itâ€™s helpful. It gives me a safety netâ€”guardrails.â€ The other 2 werenâ€™t sure.

### Purple highlights as to-doâ€™s

Several readers used the special purple highlighter in a surprising way: they marked passages which they didnâ€™t yet understand. These readers wanted to move on with the reading, but they wanted to make sure that they eventually understand the detail theyâ€™d marked. They were effectively leaving themselves a â€œto-do for understanding.â€

This makes a lot of sense! After all, I told them that if they mark a passage with their purple highlighter, the system will make sure that they internalize those ideas. The current mechanismÂ _sort of_Â accomplishes this goal. These readers received retrieval practice prompts about their â€œto-doâ€ markings. They predictably didnâ€™t know the answer, and they used the â€œView Sourceâ€ button to return to the original passage for a re-reading. In several cases, the explanation made more sense on a second pass, now that theyâ€™d seen how the ideas fit into later parts of the text.

So at least sometimes, the retrieval practice prompts indirectly accomplished these readersâ€™ â€œto-doâ€ intention, insofar as it provoked them to re-read the relevant passages. But in some cases, the passage was still confusing, and the reader needed some conversation to make sense of it. In other cases, the confusing passage didnâ€™t correspond to any retrieval practice promptâ€”for instance, one reader was confused by a particular step in an example problemâ€”and so the to-do was effectively dropped. It would be interesting to consider how one might support the â€œto-doâ€ workflow more directly.

### Transparency in highlight-to-prompt mapping

One of my â€œtargetâ€ readers felt that the highlight-to-prompt mapping was uncomfortably â€œmagicalâ€. When I asked to what extent he felt the review prompts represented his highlights, he said that he really didnâ€™t know: he couldnâ€™t easily see the correspondences, so he couldnâ€™t tell how well his intent had been reflected. The whole system felt like a black box.

This makes sense! Iâ€™m surprised more readers didnâ€™t feel this way. Technologists like to describe their products as â€œmagicalâ€, but we really want â€œmagicâ€ in the sense of â€œastounding capacity, ease, expressivityâ€, not in the sense of â€œineffable, inscrutable, mythical, eldritch.â€ My favorite paper on AI in interface design is Jeffrey Heerâ€™s 2019 â€œ[Agency plus automation](https://andymatuschak.org/files/papers/Heer%20-%202019%20-%20Agency%20plus%20automation%20Designing%20artificial%20intel.pdf)â€. In it, he argues that such interfaces benefit fromÂ _shared representations._Â You want the automated system to clearly surface its proposals in forms you yourself can create and manipulate, and you want to clearly see the connection between those proposals and the inputs which influenced them.

All that is missing from my current design. You canâ€™t write your own prompts or modify those which the system provides. During review, you can â€œView Sourceâ€ on each prompt to see which highlight it â€œcame fromâ€, but thatâ€™s a pretty cumbersome way to get an overview of the connections. And thereâ€™s no equivalent available while reading: that is, when you make a purple highlight, youâ€™re given no hint of what the system understands you to meanâ€”what prompts will result. Ideally, those representations should enable an interactive feedback loop. That is, you should be able to say â€œoh, no, thatâ€™s not what I meant; focus onÂ _this_Â part.â€

One naive solution: whenever a reader makes a purple highlight, we display a preview of the associated prompts and allow readers to intervene if they like. But I want to be careful to avoid re-introducing problemsÂ [I encountered in my prototypes late last year](https://notes.andymatuschak.org/zFcmVJAwG2YaEvJVxDM6Emc). In those designs, curated prompts were presented in the margin alongside associated content. This approach made the interaction very clear: if you click to save a prompt, youâ€™ll get exactly what you see; you can edit it in place to adjust as you like. But it also created quite a distracted reading experience. Those marginal prompts tugged readersâ€™ eyes away from the body of the text. Watching them, I could see them constantly jumping back and forth, losing their place, finding it again, eyes skipping down the page to the next spot in the text with a marginal prompt. I think most ended up spending far too much attention evaluating and making decisions about prompts.

One of my big motivations for this new highlight-centric design was to solve that problem. I wanted to make it easy for readers to remain immersed in the text, while still benefiting from selective augmentation. I think this prototype performed quite well in that regard. But Iâ€™m not yet sure how to sustain that success while creating a more transparent and shared representation for the prompts.

### Tailorable prompt mapping: emphasis notes and feedback

This prototypeâ€™s reading interface treated the highlight-to-prompt mapping as a black box, but as an experiment, it did offer a way to â€œsteerâ€ the prompts. When readers used their purple highlighter, they could optionally write a note to clarify what, specifically, theyâ€™d like to emphasize. For example, maybe youâ€™ve highlighted a definition, but itâ€™s the notation which you want to make sure you remember; or you want to make sure you internalize theÂ _contrast_Â between this definition and some earlier concept. So you can highlight the definition of (say) linear equations, and jot a note that says â€œcontrast with linear combinations.â€

Most users didnâ€™t use this feature heavily, but did use it at least once. And I can imagine that they might tend to use it more over time, as they build a mental model for how the system maps their highlights onto prompts, and for how that mapping sometimes isnâ€™t exactly what theyâ€™d want.

In practice, I was only able to honor about a third of these requests with my pre-made curated prompts. I could map another third onto broader prompts which included or indirectly reinforced the detail they mentioned. The rest of the requests were idiosyncratic enough that they probably canâ€™t be satisfied without machine-generated prompts. Only one reader noticed that his requests werenâ€™t exactly being granted, and he didnâ€™t express much concern. But I think this would become more troubling over time, and I wouldnâ€™t want to include an â€œemphasis noteâ€ feature if it isnâ€™t reliable.

The â€œemphasis noteâ€ framing front-loads user guidance. Another way to think about this kind of control is through iterative feedback. For example, one reader highlighted a theorem which says that if a linear system is transformed through one of three listed operations into another system, the second system has the same set of solutions as the first. In his review session, he got a prompt about this theoremâ€™s role in the safety of Gaussâ€™s method. He was confused about this, and once he clicked â€œView Sourceâ€ to see where the prompt came from, he said â€œOh, no, I donâ€™t really care about proving correctness hereâ€”I wanted to make sure I know the three â€œsafeâ€ operations.â€ Ideally, he should be able to justÂ _tell_Â the system that, during review: â€œJust make sure I know the safe operations."

Another reader wished that he could make the prompts less formal: more verbal explanation and examples; tone down the notation and abstraction. This kind of feedback should influence not only the current prompt but probably all the prompts in the book, and maybe all prompts in general.

My brief experiments suggest that tailoring pre-existing prompts is a much more viable task for large language models than asking them to generate prompts anew. For example, consider the prompt: â€œQ. What is theÂ _leading variable_Â of a row in a linear system? A. The first variable with a nonzero coefficient.â€ One of my readers wanted to see this kind of abstract answer in the context of an example. GPT-4Â [was able](https://chat.openai.com/share/ed8404b4-c4e6-47dc-b4d2-2e9c6b9e4aa7)Â to rewrite the prompt appropriately for that request. Of course, this example wouldnâ€™t be hard to do by hand, so the tradeoffs here may not make sense unless they can apply to many prompts, or unless the machine generation is extremely reliable.

## Next steps

Speaking more personally for a moment: this last round of testing was quite exciting for me! The new design seems to have solved many of the problems Iâ€™ve observed with my various memory system designs over the years. Andâ€”very tentativelyâ€”it also appears to help some readers with reading comprehension support. Itâ€™s a great sign thatÂ _I_Â really want to use this tool every day in my own reading.

All that said, this past round of testing was pretty shallow. I wanted to see how a variety of people reacted to the design ideas, so I met with 14 people for around an hour each. Because we were reading the first section of the book, there wasnâ€™t much opportunity for the ideas to really build on each other and put heavy demands on memory and comprehension. And because I observed just a single session, I didnâ€™t have a chance to see how the memory and comprehension support fared over time, as forgetting became more relevant.

So, starting this week, Iâ€™ll switch to a depth-first approach. Like I didÂ [earlier this year](https://notes.andymatuschak.org/zHvSKAg1fENkZwYsyuTVXq), Iâ€™ll be meeting with one student weekly for a few hours. Weâ€™ll continue more deeply into the book, where the material will start to compound in more demanding ways. Weâ€™ll also work through some problem sets during those sessions, to observe how the augmentation interacts with practical capacity.

Those observations will still be qualitative. Meanwhile, Iâ€™d like to start working towards more systematically understanding the impact of my design on reading comprehension. How often does it help readers identify meaningful gaps? Are there kinds of gaps which it tends to ignore? Do readers who use this intervention understand the material appreciably better? Feel appreciably more capable or engaged with the material?

Right now, my prototype requires me to manually map reader highlights to curated highlights. And I have to do that with little delay, so that they can click the â€œsuggested highlightsâ€ feature once they finish reading the section. If Iâ€™d like to run an experiment with a few dozen users, this would consume a huge amount of time. So, in parallel with my depth-first work with one student, Iâ€™ll attempt to automate the mapping between readersâ€™ highlights and the curated highlights.

My initial tests suggest that this is a much more tractable task to automate than the two other currently-manual tasks in my design: identifying the most important elements in a text, and writing good prompts for each of them. Itâ€™s nice that these tasks are somewhat separable, so that I can make some progress by automating just the highlight-to-prompt mapping.

And if I automate that mapping, I can make this prototype publicly available, albeit constrained to this one book. Thatâ€™s a nice milestone to work towards, and Iâ€™m sure thatâ€”as alwaysâ€”public use would surface many unexpected insights. From there, yes, it would be great to automate the curatorial and prompt-writing tasks. But Iâ€™m also interested in the prospect of using one large book as a depth-first laboratory to explore other reading augmentation ideas, many of which Iâ€™ve discussed in these essays.

One idea thatâ€™s animated my work is the claim that â€œ[books donâ€™t work](https://andymatuschak.org/books)â€. That is: in order to actually understand, internalize, and remember ideas from an explanatory text, a reader has to employ all kinds of tacit and often unreliable strategies, and the medium of the book does surprisingly little to help. What if the experience of engaging with a text naturally assured all this extra work gets done? I want to create anÂ _alien sense of capacity and ease_Â when I engage with explanatory texts.

Breaking down some of the things which you may need for a book to really â€œworkâ€ the way you might hope:

- **Comprehension.**Â You need to actually process the words on the page, and notice when youâ€™ve failed to do that.Â [This is surprisingly difficult](https://notes.andymatuschak.org/zLButXjJvGCpWKHzqhXEhhm)Â for most readers, much of the time!Â [Most interventions are quite obtrusive](https://notes.andymatuschak.org/zURU4gC8m6fr2ybewgBpvpz); the current prototype is my attempt at a more functional augmentation.
- **Memory.**Â You need to remember what you read. This motivatedÂ [Quantum Country](https://quantum.country/)Â and theÂ [mnemonic medium](https://numinous.productions/ttft). But I find that todayâ€™s memory systems often produce brittle memory, and Iâ€™d like to explore ideas likeÂ [varying prompts and escalating their complexity](https://notes.andymatuschak.org/z21VhT726p7bX8JcGM41QSA)Â to improve that.
- **Elaboration.**Â You need to understand not just what the text says, but what itÂ _means_, why thatÂ _matters_. You need to connect the textâ€™s ideas to prior knowledge and experience.Â [Discussion](https://early.khanacademy.org/open-ended/)Â is one method I like; thoughtful writing (ideally for some authentic purpose) is another. Iâ€™d like to find more, and Iâ€™d like to find ways to better connect those activities to the reading experience.
- **Fluency.**Â You need to practice using what youâ€™ve read so that it becomes automatic. Pattern induction; schema acquisition; knowledge compilation. In technical topics, this often meansÂ [problem-solving practice](https://notes.andymatuschak.org/zM6bNwCLbGwMQFdHqoEYgfr)â€”see projects likeÂ [Mathigon](https://mathigon.org/)Â andÂ [Execute Program](https://www.executeprogram.com/). Project-based learning is another common approach, and Iâ€™m interested in ideas like â€œ[doing-centric explanatory mediums](https://andymatuschak.org/doing-centric/)â€ to that end.
- **Intervention.**Â You need to diagnose and resolve confusions and misconceptions that you may have.Â [Procedurally-focused problems often fail to clearly identify conceptual issues](https://notes.andymatuschak.org/zCh4yPnzh6Y8WfCtUp9oWQw). Teaching others is a classic approach here, inspiring integrated interventions likeÂ [AutoTutor](https://notes.andymatuschak.org/AutoTutor). Frontier LLMs are often surprisingly good at resolving confusions, when the user can articulate them. Iâ€™m interested in integrated, lightweight methods for identifying and acting on confusion.
- **Integration.**Â Much of the time, you read a book not to just to acquire knowledge but because you want it toÂ _change_Â you somehowâ€”change the way you think, or the way you view the world, or the way you act or feel in a situation. To make a book real in this way, you have to carry its ideas with you into your life. For a few ideas, seeÂ [salience prompts](https://andymatuschak.org/prompts/#prompting-salience)Â andÂ [timeful texts](https://numinous.productions/timeful). Reading clubs are great for this; Iâ€™d like to explore more ideas at the intersection of new media and social convening.

Thatâ€™s enough research agenda for several lifetimes, of course. But I articulate all this here as a way of helping myself resist the cultural forces surrounding me in San Francisco. Those forces demand that if I find an ideaâ€”like highlight-driven memory promptsâ€”I should focus aggressively on scaling it to apply to as many places and people as possible. Thereâ€™s merit in that, of course! I certainly want to use this special highlighter everywhere. But I also need to weigh that impulse against the prospect of uncovering more foundational ideas, of solving the problem I care about more completely.

---

Thanks to all the students who worked with me on this first round of tests, and thanks to Benjamin Reinhardt for helpful discussion about my next steps.

# References
1. [Andy Link](https://notes.andymatuschak.org/zY3RYK9gJ6eDnq27vSwBDQh?stackedNotes=z9V2PxHVYB9p5DeCnQXcfJa)